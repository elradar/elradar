---
title: Introducción al análisis de texto
author: Ivan Recalde
date: '2019-11-16'
slug: stringr
categories: [texto]
tags: [stringr, tidy]
publishdate: '2019-11-16T20:35:12-03:00'
comments: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(wordcloud)

```

Iván Recalde, ingeniero en sistemas de la UTN, actualmente analista de datos en el Área de Gestión de Información Estadística en Salud (AGISE) hace una introducción muy clara a la visualización y procesamiento de texto libre usando tidyverse, stringr y wordcloud.


### Modelo tidy en datos de texto

Los datos no estructurados generalmente son datos que no tienen estructura interna identificable. Es un conglomerado masivo y desorganizado de varios objetos que no tienen valor hasta que se identifican y almacenan de manera organizada.

Una vez que se organizan, los elementos que conforman su contenido pueden ser buscados y categorizados (al menos hasta cierto punto) para obtener información.

```{r, echo=FALSE}
tabla_valores_no_estructurada <- dplyr::tibble(detalle = c('se realizaron 45 observaciones del tipo A en el anio 2017','se realizaron 60 observaciones del tipo B en el anio 2017','se realizaron 23 observaciones del tipo C en el anio 2017','se realizaron 32 observaciones del tipo A en el anio 2018','se realizaron 63 observaciones del tipo B en el anio 2018','se realizaron 19 observaciones del tipo C en el anio 2018'))
knitr::kable(head(tabla_valores_no_estructurada))

```



Hadley Wickham nos legó el concepto de datos en formato Tidy, cuando estos estan dispuestos con variables en las columnas y observaciones en las filas, sin que queden ni filas ni columnas con valores en blanco.

```{r, echo=FALSE}
tabla_estructurada <- dplyr::tibble(anio=c('2017','2017','2017','2018','2018','2018'), tipo = c('A','B','C','A','B','C'), n = c('45','60','23','32','63','19'))
knitr::kable(head(tabla_estructurada))
```

Esto nos permite trabajar de manera eficiente y facilmente acomodable a las funciones de... Sí, tidyverse! Nos permite saber que cada fila va a ser una observacion y que no vamos a tener una tabla con infinitas columnas. Si bien esto es excelente, el problema es que muchas veces los datos de entrada a nuestros scripts/algoritmos no vienen en  tidy, sino que los encontramos de la siguiente manera.

```{r untidy example, echo=FALSE}
tabla_no_tidy <- dplyr::tibble(anio=c('2017','2018'), tipo_a = c('45','32'), tipo_b = c('60','63'),
                               tipo_c=c('23','19'))

knitr::kable(head(tabla_no_tidy))
```

Este modelo de representacion de los datos no es peor ni mejor, como se daran cuenta suele a veces hasta ser mas rapido para visualizar observaciones en un data frame.

Teniendo estos conceptos en mente vamos a analizar texto libre.

En paralelo tal y como el modelo tidy de los datos nos permitia manejar de manera mas facil y efectiva los datos, lo mismo pasa con los datos de texto. En donde se busca que cada registro tenga uno y solo un token. Cuando hablamos de token nos referimos a lo que para nosotros es una unidad significativa de texto, en la mayoria de los casos basicos vamos a hablar de token como palabras individuales pero es importante saber que cuando el analisis es mas complejo podriamos buscar que nuestros token sean frases o parrafos enteros e intentar identifcar significado estos. Pero como pasaba con los datos estructurados normalmente no los encontramos en formato tidy, por eso una primer herramienta que vamos a ver es la 'Tokenizacion' de estos campos de texto.

### Tokenizar

Arranquemos! Vamos a trabajar con un bello poema de Tamara Grosso

```{r, include=FALSE}
poema2 <- c('ENTRE NOSOTROS:',
                    'Quisiera saber si alguna vez',
                    'se van a poder leer las mentes',
                    'Para averiguar lo que de verdad pasó entre nosotros',
                    'En tu versión del desamor')
poema2_df <- tibble(linea = 1:5, texto = poema2)


poema3 <- c('LOOP:',
            'Todavía me parece',
            'Que vas a venir un día',
            'Y me vas a decir lo que ya no quiero')
poema3_df <- tibble(linea = 1:4, texto = poema3)

poema4 <- c('VARIACIONES SOBRE LA TRISTEZA:',
            'Meter la mano en el cajón de las aspirinas',
            'y sólo encontrar el blister vacío.',
            'Un nuevo descubrimiento',
            'ni vos mismo, en el pasado,',
            'te interesaste por lo que te pasa ahora.')
poema4_df <- tibble(linea = 1:6, texto = poema4)


```

```{r}
# Tamara Grosso @tamaraestaloca cuando todo refugio se vuelve hostil @santoslocospoesia
poema <- c('ADVERTENCIA:',
                 'No se decirte',
                 'si todo va a mejorar',
                 'pero seguro la ficha',
                 'que te hizo ser quien sos',
                 'te cayo despues',
                 'de uno de los peores',
                 'dias de tu vida')
```

En este caso tenemos un vector de datos en formato character, un primer paso util seria pasarlo a un data frame, de esta manera lo traemos a un formato con el que estamos mas acostumbradxs a trabajar.

```{r pressure}
poema_df <- tibble(linea = 1:8, texto = poema)

poema_df
```


Ahora vamos a usar una funcion para tokenizar nuestro texto. En este caso el analisis lo podriamos pensar sobre palabras por separado, que en principio podrian ser nuestra unidad significativa de texto. Podriamos usar los versos?
La funcion que vamos a usar es unnest_tokens() de la biblioteca 'tidytext'. Su uso mas simple es usar los pipes de magrittr para pasarle el data frame como primer parametro implicito, luego el nombre que queremos que la columna de tokens tenga y por ultimo el nombre de la columna de origen donde deberia buscar el texto a tokenizar.

```{r}
library(tidytext)

texto_tokenizado <- poema_df %>%
  unnest_tokens(palabra_poema,texto)
texto_tokenizado

```

Veamos poner la lupa en un par de cositas bellas que nos dejo la funcion. En principio vemos que cada palabra quedo en una fila, estariamos ahora en condiciones de afirmar que cada observacion esta contenida en un registro diferente? Luego vemos que para facilitar el manejo nos transformo todos los tokens a minusculas; en el caso de no querer esto, podemos pasarle a la funcion como parametro to_lower = FALSE de la siguiente manera.

```{r}
poema_df %>%
  unnest_tokens(palabra_poema,texto, to_lower = FALSE)
```

### Completemos un circuito basico de analisis y armemos unas visualizaciones

Vamos entonces a imaginar que tenemos varios textos consecutivos (poemas en nuestro caso). Para hacer un poco mas divertido el analisis.

```{r}
varios_poemas <- poema_df %>% 
    bind_rows(poema2_df, poema3_df, poema4_df)
```
```{r, echo=FALSE}
varios_poemas <- varios_poemas %>% 
  select(-linea)
varios_poemas
```


Vamos de nuevo a tokenizar este df, como ya habiamos visto anteriormente y vamos a proceder a armar una visualizacion. Vamos a usar count() para que nos cuente cuantas ocurrencia de cada token hay, simplemente debemos decirle en que columna se encuentra. Usamos la funcion reorder(), para que luego el grafico nos muestre las barras ordenadas. Por ultimo usamos ggplot.

```{r}
library(ggplot2)

varios_poemas_tokenizados <- varios_poemas %>% 
    unnest_tokens(palabra_poema,texto)

varios_poemas_tokenizados%>%
    count(palabra_poema) %>%
    filter(n > 1) %>% 
    mutate(palabra_poema = reorder(palabra_poema, n)) %>%
    ggplot(aes(palabra_poema, n)) +
    geom_col() +
    coord_flip()
```

Otra biblioteca bastante util para visualizar de manera rapida ocurrencia de tokens (palabras), es wordcloud. Vamos a usar la funcion de base with(), para poder aplicarlo en nuestro formato con pipes de magrittr. Tenemos que tener cuidado que por defecto la minima frecuencia de aparicion es 3. 

```{r}
library(wordcloud)

varios_poemas_tokenizados %>%
  # filter(!palabra_poema %in% stopwords::stopwords(language = 'spanish')) %>% 
  count(palabra_poema) %>%
  with(wordcloud(palabra_poema, n,min.freq = 0))
```

Como vemos hay una linea comentada, que es un antijoin con stop_words, que es stop_words?

```{r, echo=FALSE}
stopwords::stopwords(language = 'spanish')[1:25]
```

Es un vector de palabras tipicas usadas en algun lenguaje que le pasemos por parametro, pero que no aportan significado al texto en la mayoria de las ocasiones. Esto sirve para que las palabras con mas ocurrencias no sean siempre las mismas sino que sean palabras significativas que aporten valor del mensaje. No lo usamos porque nuestro ejemplo tenia una cantidad muy baja de palabras y haberlo usado hubiese eliminado casi todas las palabras con mas de una ocurrencia como vemos abajo. Lo importante de todas formas es tener presente la existencia de estas colecciones de palabras.

```{r, warning=FALSE}
library(wordcloud)

varios_poemas_tokenizados %>%
  filter(!palabra_poema %in% stopwords::stopwords(language = 'spanish')) %>%
  count(palabra_poema) %>%
  with(wordcloud(palabra_poema, n,min.freq = 0))
```

### Stringr

Vamos a presentar otra herramienta muy potente Stringr. Este paquete nos propociona un conjunto de funciones para recuperar de manera sencilla informacion de texto. Esta esta construida sobre stringi, otra biblioteca mas extensa. Para explotar mas su uso y si se quedan con ganas, siempre esta bueno explorar el cheatsheet que tiene.

https://rstudio.com/resources/cheatsheets/

Vamos entonces a ver un vistazo por algunas funciones. Comencemos con algo simple, contemos cuanto caracteres tiene cada verso.

```{r}
library(stringr)

poema_df %>% 
    mutate(cantidad_caracteres = str_count(texto)) 
```

Quedemonos ahora solo con una parte del texto, en este caso los primeros 5 caracteres. Veamos que a nivel grafico nos muestra el resultado con comillas para denotar que quedo un espacio [' '], al principio o al final.

```{r}
poema_df %>% 
    mutate(solo_primeros_cinco = str_sub(texto,1,5)) 
```

Las posiciones son relativas al largo del texto, podemos entonces decirle que agarre los ultimos 5 caracteres de la siguiente manera

```{r}
poema_df %>% 
    mutate(solo_primeros_cinco = str_sub(texto,-5,-1)) 
```

Otro uso tipico es querer modificar todo a minusculas, pero como sobre gustos no hay nada definido tambien nos permite modificar todo a mayusculas.

```{r}
poema_df %>% 
    mutate(mayusculas = str_to_upper(texto),
           minusculas = str_to_lower(texto)) 
```

### str_detect()

Veamos ahora como identificar patrones dentro del texto libre desde su manera mas sencilla y veamos algunos ejemplos que pueden servir como disparadores. 
Generemos una columna nueva que nos diga si este patron estaba en el texto de cada registro.

```{r}
poema_df %>% 
    mutate(tengo_de = str_detect(texto, 'de')) 
```

Podriamos tambien querer filtrar y quedarnos solo con las ocurrencias de este patron.

```{r}
poema_df %>% 
    filter(str_detect(texto, 'de')) 
```

Volvamos ahora al caso donde teniamos todos los poemas juntos, seria posible identificar de alguna manera cada poema por separado?
```{r, echo=FALSE}
varios_poemas
```

Busquemos entonces generar un corte cada vez que encuentre los ':', que es en este caso por lo menos lo que nos indica que hay un titulo. Una funcion que nos podria servir es cumsum(), que nos va a mantener un contador cada vez que se cumpla una condicion que le pasemos por parametro.

```{r}

poemas_separados <- varios_poemas %>%
  mutate(poema = cumsum(str_detect(texto, ':')))

poemas_separados
```

Excelente, esto nos permitira abstraernos de la cantidad de registros que tengamos tokenizados por cada archivo de texto original.

### Como recuperar lo que partimos? [lo rompi?]

Una vez que aprendimos a separar [romper] algo, estaria buenisimo tambien saber volverlo a armar, verdad?
Vamos a usar la funcion paste() y le vamos a pasar por parametro collapse, para definirle que queremos que nos deje en el medio de cada verso del poema en este caso.

```{r}
poemas_unidos <- poemas_separados %>% 
    group_by(poema) %>% 
    mutate(poema_entero = paste(texto,
                           collapse = ' ')) %>% 
    slice(1) %>% 
    ungroup() %>% 
    select(poema_entero)

poemas_unidos
```

### Proximos pasos -> str_extract() y regex()

Para ir finalizando con estas herramientas basicas, volvamos al ejemplo original que usamos para ver como podiamos encontrarnos la informacion no estructurada

```{r, echo=FALSE}
knitr::kable(head(tabla_valores_no_estructurada))

```

Vamos a usar str_extract para obtener la informacion que esta perdida dentro del campo libre, pero para que esta funcion realmente explote su potencia va a necesitar que le agreguemos expresiones regulares. Las expresiones pueden ir desde algo muy simple, hasta algo super complejo, podemos ayudarnos del cheatsheet y de paginas como https://regex101.com/ que nos permiten en tiempo real ir probando nuestras expresiones. Vamos a ver como a priori este caso se resuelve con expresiones bastante amigables

```{r}
tabla_valores_no_estructurada %>% 
    mutate(n = str_extract(detalle, regex('[0-9]+')), #uno o mas numeros
           tipo = str_extract(detalle, regex('[ABC] ')), #letras en mayuscula A, B o C
           anio = str_extract(detalle, regex('[0-9]+$'))) #uno o mas numeros y fin de texto
```

Algo interesante a tener en cuenta es observar como la primera expresion regular no trae el anio ademas del n. Esto se debe a que salvo que le indiquemos lo contrario str_extract(), nos trae solo lo primero que encuentra. Entonces una vez que encuentra uno o mas numeros seguidos, deja de mirar el texto.

### Conclusion

Este pequeño tutorial tiene como finalidad presentar un vistazo rapido por bastantes herramientas para el analisis de texto. Ojala sirva como disparador para investigar mas! 